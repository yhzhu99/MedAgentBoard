from openai import OpenAI
from typing import Any, Optional
from .llm_configs import LLM_MODELS_SETTINGS

def vqa_rad_ff_prompt(question, ground_truth, model_answer):
    """
    Generates a prompt for an LLM to evaluate the binary correctness 
    of a model's answer against a ground truth, specific to VQA-RAD context.
    """
    
    system_prompt = """
    **You are a Medical Expert specialized in questions associated with radiological images. Your task is to act as an impartial judge and evaluate the correctness of an AI model's response to a medical visual question.**
    """
    
    user_prompt = f"""
    **Inputs You Will Receive:**

    1.  **Question:** The question asked, likely referring to an (unseen) medical image.
    2.  **Ground Truth Answer:** The accepted correct answer based on the image and question.
    3.  **Model's Answer:** The answer generated by the AI model you need to evaluate.

    **Evaluation Dimension: Binary Correctness**

    Assess whether the **Model's Answer** is essentially correct when compared to the **Ground Truth Answer**, considering the **Question**.

    **Criteria:**

    *   **1:** The **Model's Answer** is essentially correct. It accurately answers the **Question** and aligns with the core meaning of the **Ground Truth Answer**. Minor phrasing differences are acceptable if the core meaning is preserved.
    *   **0:** The **Model's Answer** is incorrect. It fails to answer the **Question** accurately, or significantly contradicts the **Ground Truth Answer**.

    **Output Requirement:**

    **Output ONLY the single digit '1' (if correct) or '0' (if incorrect).** Do NOT provide any justification, explanation, or any other text. Your entire response must be just the single digit '1' or '0'.

    **Evaluation Task:**

    **Question:** {question}
    **Ground Truth Answer:** {ground_truth}
    **Model's Answer:** {model_answer}

    """

    return system_prompt, user_prompt

def pubmedqa_ff_prompt(question: str, ground_truth: str, model_answer: Any) -> tuple[str, str]:
    """
    Generates a prompt for an LLM to act as a critical medical expert judge,
    evaluating the correctness and quality of a model's answer based *only* 
    on the question and a ground truth answer, using a 1-10 scale and 
    requiring JSON output with detailed reasoning.
    """
    
    # Ensure model_answer is a string for insertion into the prompt
    if not isinstance(model_answer, str):
        try:
            model_answer_str = json.dumps(model_answer) 
        except Exception:
            model_answer_str = str(model_answer)
    else:
        model_answer_str = model_answer

    system_prompt = """
    **You are a highly knowledgeable and critical Medical Expert. Your task is to act as an impartial judge and rigorously evaluate the quality and correctness of an AI model's response to a medical question. You will assess this *solely* by comparing the model's response to the provided Ground Truth Answer, considering the original Question.**
    """
    
    user_prompt = f"""
    **Inputs You Will Receive:**

    1.  **Question:** The original question asked.
    2.  **Ground Truth Answer:** The reference answer, considered correct and complete for the given question. This is your primary standard for evaluation.
    3.  **Model's Response:** The answer generated by the AI model you must evaluate.

    **Evaluation Dimension: Correctness and Alignment with Ground Truth**

    Assess the **Model's Response** based *only* on its factual accuracy, completeness, relevance, and overall alignment compared to the **Ground Truth Answer**, considering the scope of the **Question**.

    *   **Factual Accuracy & Alignment:** Does the information presented in the **Model's Response** accurately reflect the information in the **Ground Truth Answer**? Are the key facts, conclusions, and nuances the same? Identify any contradictions, inaccuracies, or misrepresentations compared to the ground truth.
    *   **Completeness:** Does the **Model's Response** cover the essential information present in the **Ground Truth Answer** needed to fully address the **Question**? Note significant omissions of key details found in the ground truth.
    *   **Relevance & Conciseness:** Is all information in the **Model's Response** relevant to answering the **Question**, as exemplified by the **Ground Truth Answer**? Penalize irrelevant information, excessive verbosity, or details not present in the ground truth that don't enhance the answer's quality. **Focus on the accuracy and completeness relative to the ground truth, not length.**
    *   **Overall Semantic Equivalence:** Does the **Model's Response** convey the same meaning and conclusion as the **Ground Truth Answer**, even if phrased differently?

    **Scoring Guide (1-10 Scale):**

    *   **10: Perfect Match:** The answer is factually identical or perfectly semantically equivalent to the ground truth. It fully answers the question accurately and concisely, mirroring the ground truth's content and conclusion.
    *   **9: Excellent Alignment:** Minor phrasing differences from the ground truth, but all key facts and the conclusion are perfectly represented. Negligible, harmless deviations.
    *   **8: Very Good Alignment:** Accurately reflects the main points and conclusion of the ground truth. May omit very minor details from the ground truth or have slightly different phrasing, but the core meaning is identical.
    *   **7: Good Alignment:** Captures the core message and conclusion of the ground truth correctly. May omit some secondary details present in the ground truth or contain minor inaccuracies that don't significantly alter the main point.
    *   **6: Mostly Fair Alignment:** Addresses the question and aligns with the ground truth's main conclusion, but contains noticeable factual discrepancies compared to the ground truth or omits important details found in the ground truth.
    *   **5: Fair Alignment:** Contains a mix of information that aligns and contradicts the ground truth. May get the general idea but includes significant errors or omissions when compared to the ground truth. The conclusion might be partially correct but poorly represented.
    *   **4: Mostly Poor Alignment:** Attempts to answer the question but significantly deviates from the ground truth in facts or conclusion. Misses key information from the ground truth or introduces substantial inaccuracies.
    *   **3: Poor Alignment:** Largely incorrect compared to the ground truth. Shows a fundamental misunderstanding or misrepresentation of the information expected based on the ground truth.
    *   **2: Very Poor Alignment:** Almost entirely incorrect or irrelevant when compared to the ground truth. Fails to address the question meaningfully in a way that aligns with the expected answer.
    *   **1: No Alignment/Incorrect:** Completely incorrect, irrelevant, or contradicts the ground truth entirely. Offers no valid information related to the question based on the ground truth standard.

    **Output Requirement:**

    **Output ONLY a single JSON object** in the following format. Do NOT include any text before or after the JSON object. Ensure your reasoning specifically compares the Model's Response to the Ground Truth Answer.

    ```json
    {{
      "reasoning": "Provide your step-by-step thinking process here. \n1. Compare Content: Directly compare the facts, details, and conclusions in the 'Model's Response' against the 'Ground Truth Answer'. Note specific points of alignment, discrepancy, omission, or addition. \n2. Assess Relevance & Completeness: Evaluate if the 'Model's Response' fully addresses the 'Question' as comprehensively as the 'Ground Truth Answer' does. Is there irrelevant content not present or implied by the ground truth? \n3. Evaluate Semantic Equivalence: Does the model's answer mean the same thing as the ground truth? \n4. Final Assessment & Score Justification: Synthesize the comparison. Explicitly state why the assigned score is appropriate based on the rubric, highlighting the degree of match/mismatch between the Model's Response and the Ground Truth.",
      "score": <The final numerical score (integer between 1 and 10)>
    }}
    ```
    
    **Evaluation Task:**

    **Question:** {question}
    **Ground Truth Answer:** {ground_truth}
    **Model's Response:** {model_answer_str}
    """
    
    return system_prompt, user_prompt

def llm_score(
        question: str, 
        ground_truth: str, 
        model_answer: str, 
        dataset: str, 
        model_key:str):
    """
    Evaluates the correctness of a model's answer against a ground truth
    using an LLM. The function generates a prompt based on the dataset type
    and returns the score given by the LLM.
    """
    
    if dataset == "VQA-RAD":
        system_prompt, user_prompt = vqa_rad_ff_prompt(question, ground_truth, model_answer)
    elif dataset == "PubMedQA":
        system_prompt, user_prompt = pubmedqa_ff_prompt(question, ground_truth, model_answer)
    else:
        raise ValueError(f"Unsupported dataset: {dataset}")
    
    # Call the LLM with the generated prompt   
    messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]
    
    model_settings = LLM_MODELS_SETTINGS[model_key]
    client = OpenAI(api_key=model_settings["api_key"],base_url=model_settings["base_url"])
    
    response = client.chat.completions.create(
        model=model_settings["model_name"],
        messages=messages,
        stream=False
    )
    
    return response.choices[0].message.content